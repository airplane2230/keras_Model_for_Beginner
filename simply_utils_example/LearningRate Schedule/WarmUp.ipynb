{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedule 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, init_lr, warmup_step, decay_fn):\n",
    "        self.init_lr = init_lr\n",
    "        self.warmup_step = warmup_step\n",
    "        self.decay_fn = decay_fn\n",
    "\n",
    "    def __call__(self, step):\n",
    "        if step == 0:\n",
    "            step += 1\n",
    "            \n",
    "        step_float = tf.cast(step, tf.float32)\n",
    "        warmup_step_float = tf.cast(self.warmup_step, tf.float32)\n",
    "\n",
    "        return tf.cond(\n",
    "            step_float < warmup_step_float,\n",
    "            lambda: init_lr * (step_float / warmup_step_float),\n",
    "            lambda: self.decay_fn(step_float - warmup_step_float),\n",
    "        )\n",
    "\n",
    "# data_size: train_set 크기\n",
    "data_size = 100000; batch_size = 512\n",
    "global_step = data_size // batch_size\n",
    "warmup_step = int(global_step * 0.6)\n",
    "init_lr = 0.1\n",
    "min_lr = 1e-6\n",
    "power = 1.\n",
    "    \n",
    "lr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate = init_lr,\n",
    "    decay_steps = global_step - warmup_step,\n",
    "    end_learning_rate = min_lr,\n",
    "    power = power\n",
    ")\n",
    "\n",
    "lr_schedule = LRSchedule(init_lr, warmup_step, lr_scheduler)\n",
    "\n",
    "# 사용 예시\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRSchedule(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, init_lr, warmup_epoch, decay_fn):\n",
    "        self.init_lr = init_lr\n",
    "        self.decay_fn = decay_fn\n",
    "        self.warmup_epoch = warmup_epoch\n",
    "        self.lrs = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs = None):\n",
    "        if epoch == 0:\n",
    "            epoch += 1\n",
    "\n",
    "        global_epoch = tf.cast(epoch, tf.float32)\n",
    "        warmup_epoch_float = tf.cast(self.warmup_epoch, tf.float32)\n",
    "\n",
    "        lr = tf.cond(\n",
    "                global_epoch < warmup_epoch_float,\n",
    "                lambda: init_lr * (global_epoch / warmup_epoch_float),\n",
    "                lambda: self.decay_fn(global_epoch - warmup_epoch_float),\n",
    "                )\n",
    "\n",
    "        tf.print('learning rate: ', lr)\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "        \n",
    "        self.lrs.append(lr)\n",
    "        \n",
    "        \n",
    "epochs = 1000\n",
    "warmup_epoch = int(epochs * 0.4)\n",
    "init_lr = 0.1\n",
    "min_lr = 1e-6\n",
    "power = 1.\n",
    "    \n",
    "lr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate = init_lr,\n",
    "    decay_steps = epochs - warmup_epoch,\n",
    "    end_learning_rate = min_lr,\n",
    "    power = power\n",
    ")\n",
    "\n",
    "# lr_schedule = LRSchedule(init_lr = init_lr,\n",
    "#                          warmup_epoch = warmup_epoch,\n",
    "#                          decay_fn = lr_scheduler)\n",
    "\n",
    "# for i in range(epochs):\n",
    "#     lr_schedule.on_epoch_begin(i)\n",
    "\n",
    "# 사용 예시\n",
    "model.fit(..., callbacks = [LRSchedule(init_lr = init_lr,\n",
    "                                      warmup_epoch = warmup_epoch,\n",
    "                                      decay_fn = lr_scheduler)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "plt.plot(lr_schedule.lrs)\n",
    "plt.xlabel('epochs', fontsize = 16)\n",
    "plt.ylabel('learning rate', fontsize = 16)\n",
    "plt.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jhw",
   "language": "python",
   "name": "jhw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
