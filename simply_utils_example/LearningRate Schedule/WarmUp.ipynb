{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schedule 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, init_lr, warmup_epoch,\n",
    "                 steps_per_epoch,\n",
    "                 decay_fn, *,\n",
    "                 continue_epoch = 0):\n",
    "        self.init_lr = init_lr\n",
    "        self.decay_fn = decay_fn\n",
    "        self.warmup_epoch = warmup_epoch\n",
    "        self.continue_epoch = continue_epoch\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.lr = 1e-4 # remove\n",
    "\n",
    "    def on_epoch_begin(self, epoch):\n",
    "        epoch = tf.cast(epoch, tf.float64)\n",
    "        \n",
    "        global_epoch = tf.cast(epoch + 1, tf.float64)\n",
    "        warmup_epoch_float = tf.cast(self.warmup_epoch, tf.float64)\n",
    "        \n",
    "        lr = tf.cond(\n",
    "            global_epoch < warmup_epoch_float,\n",
    "            lambda: tf.cast(self.init_lr * (global_epoch / warmup_epoch_float), tf.float64),\n",
    "            lambda: tf.cast(self.decay_fn(epoch - warmup_epoch_float), tf.float64)\n",
    "        )\n",
    "        self.lr = lr\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        def compute_epoch(step):\n",
    "            return step // self.steps_per_epoch\n",
    "        \n",
    "        epoch = compute_epoch(step)\n",
    "        epoch = epoch + self.continue_epoch\n",
    "        \n",
    "        self.on_epoch_begin(epoch)\n",
    "        \n",
    "        return self.lr\n",
    "\n",
    "def get_steps(x_size, batch_size):\n",
    "    if x_size / batch_size == 0:\n",
    "        return x_size // batch_size\n",
    "    else:\n",
    "        return x_size // batch_size + 1\n",
    "\n",
    "# data_size: train_set 크기\n",
    "data_size = 100000\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 100\n",
    "warmup_epoch = int(EPOCHS * 0.1)\n",
    "init_lr = 0.1\n",
    "min_lr = 1e-6\n",
    "power = 1.\n",
    "    \n",
    "lr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate = init_lr,\n",
    "    decay_steps = EPOCHS - warmup_epoch,\n",
    "    end_learning_rate = min_lr,\n",
    "    power = power\n",
    ")\n",
    "\n",
    "# get_steps: 에폭당 step 수 전달\n",
    "lr_schedule = LRSchedule(init_lr, warmup_epoch,\n",
    "                         steps_per_epoch = get_steps(data_size, BATCH_SIZE),\n",
    "                         decay_fn = lr_scheduler,\n",
    "                         continue_epoch = 0)\n",
    "\n",
    "# 사용 예시\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRSchedule(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, init_lr, warmup_epoch, decay_fn):\n",
    "        self.init_lr = init_lr\n",
    "        self.decay_fn = decay_fn\n",
    "        self.warmup_epoch = warmup_epoch\n",
    "        self.lrs = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs = None):\n",
    "        global_epoch = tf.cast(epoch + 1, tf.float64)\n",
    "        warmup_epoch_float = tf.cast(self.warmup_epoch, tf.float64)\n",
    "\n",
    "        lr = tf.cond(\n",
    "                global_epoch < warmup_epoch_float,\n",
    "                lambda: init_lr * (global_epoch / warmup_epoch_float),\n",
    "                lambda: self.decay_fn(global_epoch - warmup_epoch_float),\n",
    "                )\n",
    "\n",
    "        tf.print('learning rate: ', lr)\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "        \n",
    "        self.lrs.append(lr)\n",
    "        \n",
    "        \n",
    "epochs = 1000\n",
    "warmup_epoch = int(epochs * 0.1)\n",
    "init_lr = 0.1\n",
    "min_lr = 1e-6\n",
    "power = 1.\n",
    "    \n",
    "lr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate = init_lr,\n",
    "    decay_steps = epochs - warmup_epoch,\n",
    "    end_learning_rate = min_lr,\n",
    "    power = power\n",
    ")\n",
    "\n",
    "# lr_schedule = LRSchedule(init_lr = init_lr,\n",
    "#                          warmup_epoch = warmup_epoch,\n",
    "#                          decay_fn = lr_scheduler)\n",
    "\n",
    "# for i in range(epochs):\n",
    "#     lr_schedule.on_epoch_begin(i)\n",
    "\n",
    "# 사용 예시\n",
    "model.fit(..., callbacks = [LRSchedule(init_lr = init_lr,\n",
    "                                      warmup_epoch = warmup_epoch,\n",
    "                                      decay_fn = lr_scheduler)],\n",
    "          initial_epoch = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "plt.plot(lr_schedule.lrs)\n",
    "plt.xlabel('epochs', fontsize = 16)\n",
    "plt.ylabel('learning rate', fontsize = 16)\n",
    "plt.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
