{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make TF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datasets(x, y):\n",
    "    # (28, 28) -> (28, 28, 1)\n",
    "    def _new_axis(x, y):\n",
    "        y = tf.one_hot(y, depth = 10)\n",
    "        \n",
    "        return x[..., tf.newaxis], y\n",
    "            \n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    ds = ds.map(_new_axis, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.shuffle(100).batch(32) # 배치 크기 조절하세요\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return ds\n",
    "    \n",
    "ds = make_datasets(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescaling, 1 / 255\n",
    "preprocessing_layer = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.Rescaling(1./255),\n",
    "    ])\n",
    "\n",
    "# simple CNN model\n",
    "def get_model():\n",
    "    inputs = Input(shape = (28, 28, 1))\n",
    "    preprocessing_inputs = preprocessing_layer(inputs)\n",
    "    \n",
    "    x = Conv2D(filters = 32, kernel_size = (3, 3), activation='relu')(preprocessing_inputs)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(filters = 64, kernel_size = (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(filters = 64, kernel_size =(3, 3), activation='relu')(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation = 'relu')(x)\n",
    "    outputs = Dense(10, activation = 'softmax')(x)\n",
    "    \n",
    "    model = Model(inputs = inputs, outputs = outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Gradient Accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "num_accum = 4 # 누적 횟수\n",
    "\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits = True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss_value = loss_fn(y, logits)\n",
    "    gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "    \n",
    "    # update metrics    \n",
    "    train_acc_metric.update_state(y, logits)\n",
    "    \n",
    "    return gradients, loss_value\n",
    "\n",
    "def train():\n",
    "    for epoch in range(epochs):\n",
    "        print(f'################ Start of epoch: {epoch} ################')\n",
    "        # 누적 gradient를 담기 위한 zeros_like 선언\n",
    "        accumulation_gradients = [tf.zeros_like(ele) for ele in model.trainable_weights]\n",
    "        \n",
    "        for step, (batch_x_train, batch_y_train) in enumerate(ds):\n",
    "            gradients, loss_value = train_step(batch_x_train, batch_y_train)\n",
    "            \n",
    "            if step % num_accum == 0:\n",
    "                accumulation_gradients = [grad / num_accum for grad in accumulation_gradients]\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "                # zero-like init\n",
    "                accumulation_gradients = [tf.zeros_like(ele) for ele in model.trainable_weights]\n",
    "            else:\n",
    "                accumulation_gradients = [(accum_grad + grad) for accum_grad, grad in zip(accumulation_gradients, gradients)]\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(f\"Loss at Step: {step} : {loss_value:.4f}\")\n",
    "            \n",
    "        train_acc = train_acc_metric.result()\n",
    "        print(f'Accuracy : {(train_acc * 100):.4f}%')\n",
    "        train_acc_metric.reset_states()\n",
    "        \n",
    "# start training\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jhw",
   "language": "python",
   "name": "jhw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
