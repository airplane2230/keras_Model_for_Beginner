{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow as tf\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Schedule 버전"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class LRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\r\n",
    "    def __init__(self, init_lr, warmup_epoch,\r\n",
    "                 steps_per_epoch,\r\n",
    "                 decay_fn, *,\r\n",
    "                 continue_epoch = 0):\r\n",
    "        self.init_lr = init_lr\r\n",
    "        self.decay_fn = decay_fn\r\n",
    "        self.warmup_epoch = warmup_epoch\r\n",
    "        self.continue_epoch = continue_epoch\r\n",
    "        self.steps_per_epoch = steps_per_epoch\r\n",
    "        self.lr = 1e-4 # remove\r\n",
    "\r\n",
    "    def on_epoch_begin(self, epoch):\r\n",
    "        epoch = tf.cast(epoch, tf.float64)\r\n",
    "        \r\n",
    "        global_epoch = tf.cast(epoch + 1, tf.float64)\r\n",
    "        warmup_epoch_float = tf.cast(self.warmup_epoch, tf.float64)\r\n",
    "        \r\n",
    "        lr = tf.cond(\r\n",
    "            global_epoch < warmup_epoch_float,\r\n",
    "            lambda: tf.cast(self.init_lr * (global_epoch / warmup_epoch_float), tf.float64),\r\n",
    "            lambda: tf.cast(self.decay_fn(epoch - warmup_epoch_float), tf.float64)\r\n",
    "        )\r\n",
    "        self.lr = lr\r\n",
    "    \r\n",
    "    def __call__(self, step):\r\n",
    "        def compute_epoch(step):\r\n",
    "            return step // self.steps_per_epoch\r\n",
    "        \r\n",
    "        epoch = compute_epoch(step)\r\n",
    "        epoch = epoch + self.continue_epoch\r\n",
    "        \r\n",
    "        self.on_epoch_begin(step)\r\n",
    "        \r\n",
    "        return self.lr\r\n",
    "\r\n",
    "def get_steps(x_size, batch_size):\r\n",
    "    if x_size / batch_size == 0:\r\n",
    "        return x_size // batch_size\r\n",
    "    else:\r\n",
    "        return x_size // batch_size + 1\r\n",
    "\r\n",
    "# data_size: train_set 크기\r\n",
    "data_size = 100000\r\n",
    "BATCH_SIZE = 512\r\n",
    "EPOCHS = 100\r\n",
    "warmup_epoch = int(EPOCHS * 0.1)\r\n",
    "init_lr = 0.1\r\n",
    "min_lr = 1e-6\r\n",
    "power = 1.\r\n",
    "    \r\n",
    "lr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\r\n",
    "    initial_learning_rate = init_lr,\r\n",
    "    decay_steps = EPOCHS - warmup_epoch,\r\n",
    "    end_learning_rate = min_lr,\r\n",
    "    power = power\r\n",
    ")\r\n",
    "\r\n",
    "lr_schedule = LRSchedule(init_lr, warmup_epoch,\r\n",
    "                         steps_per_epoch = get_steps(data_size, BATCH_SIZE),\r\n",
    "                         decay_fn = lr_scheduler,\r\n",
    "                         continue_epoch = 0)\r\n",
    "\r\n",
    "# 사용 예시\r\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = lr_schedule)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Callback 버전"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class LRSchedule(tf.keras.callbacks.Callback):\r\n",
    "    def __init__(self, init_lr, warmup_epoch, decay_fn):\r\n",
    "        self.init_lr = init_lr\r\n",
    "        self.decay_fn = decay_fn\r\n",
    "        self.warmup_epoch = warmup_epoch\r\n",
    "        self.lrs = []\r\n",
    "\r\n",
    "    def on_epoch_begin(self, epoch, logs = None):\r\n",
    "        global_epoch = tf.cast(epoch + 1, tf.float64)\r\n",
    "        warmup_epoch_float = tf.cast(self.warmup_epoch, tf.float64)\r\n",
    "\r\n",
    "        lr = tf.cond(\r\n",
    "                global_epoch < warmup_epoch_float,\r\n",
    "                lambda: init_lr * (global_epoch / warmup_epoch_float),\r\n",
    "                lambda: self.decay_fn(global_epoch - warmup_epoch_float),\r\n",
    "                )\r\n",
    "\r\n",
    "        tf.print('learning rate: ', lr)\r\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, lr)\r\n",
    "        \r\n",
    "        self.lrs.append(lr)\r\n",
    "        \r\n",
    "        \r\n",
    "epochs = 1000\r\n",
    "warmup_epoch = int(epochs * 0.1)\r\n",
    "init_lr = 0.1\r\n",
    "min_lr = 1e-6\r\n",
    "power = 1.\r\n",
    "    \r\n",
    "lr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\r\n",
    "    initial_learning_rate = init_lr,\r\n",
    "    decay_steps = epochs - warmup_epoch,\r\n",
    "    end_learning_rate = min_lr,\r\n",
    "    power = power\r\n",
    ")\r\n",
    "\r\n",
    "# lr_schedule = LRSchedule(init_lr = init_lr,\r\n",
    "#                          warmup_epoch = warmup_epoch,\r\n",
    "#                          decay_fn = lr_scheduler)\r\n",
    "\r\n",
    "# for i in range(epochs):\r\n",
    "#     lr_schedule.on_epoch_begin(i)\r\n",
    "\r\n",
    "# 사용 예시\r\n",
    "model.fit(..., callbacks = [LRSchedule(init_lr = init_lr,\r\n",
    "                                      warmup_epoch = warmup_epoch,\r\n",
    "                                      decay_fn = lr_scheduler)],\r\n",
    "          initial_epoch = 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize = (10, 10))\r\n",
    "plt.plot(lr_schedule.lrs)\r\n",
    "plt.xlabel('epochs', fontsize = 16)\r\n",
    "plt.ylabel('learning rate', fontsize = 16)\r\n",
    "plt.grid()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jhw",
   "language": "python",
   "name": "jhw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}